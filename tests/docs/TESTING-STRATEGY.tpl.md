# Universal Template System - Unknown Stack
# Generated: 2025-12-10
# Purpose: Testing utilities
# Tier: base
# Stack: unknown
# Category: testing

# Universal Testing Strategy & Implementation Guide

**Purpose**: Technology-agnostic testing strategy covering all test types: Unit Tests, Component/UI Tests, Integration Tests, Feature Tests, Workflow Tests, System Tests, and E2E Tests. This document provides universal testing philosophy, organization structure, and best practices applicable to any software project.

**Last Updated**: 2025-12-09
**Version**: 2.1 (Three Pillars Framework)
**Three Pillars**: Scripting, Testing, Documenting  
**Framework**: Universal - All Languages & Platforms  
**Test Types**: 7-Layer Testing Strategy

---

## ğŸ“‹ Table of Contents

1. [Testing Strategy Overview](#testing-strategy-overview)
2. [Test Types & Specifications](#test-types--specifications)
3. [Test Organization & Structure](#test-organization--structure)
4. [Coverage Requirements](#coverage-requirements)
5. [CI/CD Integration](#cicd-integration)
6. [Implementation Guidelines](#implementation-guidelines)
7. [Test Debugging & Maintenance](#test-debugging--maintenance)
8. [Technology-Specific Implementations](#technology-specific-implementations)

---

## ğŸ¯ Testing Strategy Overview

### **Testing Philosophy - Three Pillars Framework**

**Test Everything That Matters** (ğŸ§ª TESTING Pillar):
- Every feature must be tested
- Every workflow must be validated
- Every user journey must work correctly
- Performance must be measured
- Errors must be handled gracefully
- **Three Pillars Integration**: Testing validates both Scripting (automation) and Documenting (accuracy)

**Test Early, Test Often** (Three Pillars Approach):
- Write tests during development (TDD preferred)
- Run tests continuously during development
- No code without tests
- No merge without passing tests
- **Scripting Integration**: Automated test execution via `.\scripts\ai-workflow.ps1`
- **Documentation Integration**: Test results inform documentation updates

**Test Quality Metrics** (Three Pillars Standards):
- **Coverage**: 85%+ overall minimum (ğŸ§ª TESTING)
- **Reliability**: Zero flaky tests
- **Maintainability**: Clear, readable, well-documented tests (ğŸ“š DOCUMENTING)
- **Performance**: All tests run in under 5 minutes
- **Automation**: Integrated with `.\scripts\ai-workflow.ps1` (ğŸ¯ SCRIPTING)

---

### **Test Types Overview**

| Test Type | Level | Purpose | Scope | Speed | CI/CD |
|-----------|-------|---------|-------|-------|-------|
| **Unit Tests** | 1 | Individual functions/methods | Single unit | Fastest | âœ… |
| **Component Tests** | 2 | UI components in isolation | Single component | Fast | âœ… |
| **Integration Tests** | 3 | Component interactions | Multiple components | Medium | âœ… |
| **Feature Tests** | 4 | Complete features end-to-end | Feature module | Medium | âœ… |
| **Workflow Tests** | 5 | User workflows across features | Cross-feature journeys | Medium-Slow | âœ… |
| **System Tests** | 6 | Entire system with platform | Full system + platform | Slow | âš ï¸ |
| **E2E Tests** | 7 | Production-like scenarios | Real environment | Slowest | â³ |

---

## ğŸ§ª Test Types & Specifications

### **1. Unit Tests (Foundation)**

**Purpose**: Test individual functions, methods, and classes in complete isolation from external dependencies.

**Scope**:
- Business logic in use cases/services
- Repository implementations
- Utility functions and helpers
- Data model validation
- Algorithm implementations
- Pure functions

**Characteristics**:
- Fast execution (< 10ms per test)
- No external dependencies (fully mocked)
- Tests a single unit of code in isolation
- High coverage required (90%+)
- Run frequently during development (watch mode)

**Principles**:
- **Arrange-Act-Assert**: Clear test structure
- **Test Independence**: Each test can run alone or in any order
- **Single Responsibility**: Each test verifies one behavior
- **Descriptive Names**: Test names should read like specifications

**When to Write**:
- âœ… For every public function/method
- âœ… For all business logic
- âœ… For data transformations
- âœ… For validation logic
- âœ… For error handling paths
- âœ… For edge cases and boundary conditions

**When NOT to Write**:
- âŒ Private methods (test through public API)
- âŒ Auto-generated code
- âŒ Third-party library code
- âŒ Simple getters/setters (unless complex logic)

---

### **2. Component/UI Tests (Presentation Layer)**

**Purpose**: Test individual UI components in isolation, verifying rendering, user interactions, and state management.

**Scope**:
- Single UI components
- Rendering with various props/data
- User interactions (clicks, input, gestures)
- State changes and re-rendering
- Event handling
- Visual states (loading, error, empty)

**Characteristics**:
- Tests UI in isolation from business logic
- Simulates user interactions
- Verifies visual elements and state
- Medium-fast execution (< 100ms)
- Coverage target: 80%+

**Framework-Specific Notes**:
- **Flutter**: "Widget Tests" - Test widgets with `flutter_test`
- **React**: "Component Tests" - Test components with React Testing Library
- **Vue**: "Component Tests" - Test components with Vue Test Utils
- **Angular**: "Component Tests" - Test components with Angular Testing Library
- **Native Mobile**: "UI Tests" - Test native UI components

**When to Write**:
- âœ… For every reusable UI component
- âœ… For user interaction handling
- âœ… For state-driven UI changes
- âœ… For forms and input validation
- âœ… For navigation elements
- âœ… For error/success/loading states

**When NOT to Write**:
- âŒ Business logic (move to unit tests)
- âŒ API calls (mock responses)
- âŒ Navigation flows (use integration/feature tests)

---

### **3. Integration Tests (Component Collaboration)**

**Purpose**: Test interactions between multiple components, services, and layers to verify they work together correctly.

**Scope**:
- Database operations
- API calls and responses
- Repository with real database
- Services with multiple dependencies
- Multi-step operations
- Platform features (camera, file I/O, permissions)

**Characteristics**:
- Tests component collaboration
- Uses real implementations with test configurations
- May use in-memory/test databases
- Slower execution (< 500ms)
- Coverage target: 70%+

**When to Write**:
- âœ… For database CRUD operations
- âœ… For repository implementations
- âœ… For service layer integration
- âœ… For multi-step business operations
- âœ… For platform integration (camera, storage, etc.)
- âœ… For authentication/authorization flows

**When NOT to Write**:
- âŒ Single function testing (use unit tests)
- âŒ UI-only testing (use component tests)
- âŒ Complete user journeys (use feature/workflow tests)

---

### **4. Feature Tests (End-to-End Feature Validation)**

**Purpose**: Test complete features end-to-end as cohesive units, validating all user actions within a feature work correctly.

**Definition**: A feature is a cohesive set of functionality that delivers value to the user. Examples:
- **Item Management Feature**: Add, edit, delete, view items
- **Search Feature**: Search, filter, sort results
- **Import/Export Feature**: CSV import/export with validation
- **Authentication Feature**: Login, logout, password reset
- **Payment Feature**: Complete purchase flow

**Scope**:
- All user actions within a feature
- All screens/components of a feature
- Feature-specific validation logic
- Feature error handling
- Feature edge cases
- Performance within feature boundaries

**Characteristics**:
- Duration: 1-5 seconds per test
- Scope: Complete feature, multiple screens/components
- Isolation: Feature isolated from other features (internal components work together)
- Data: Test data factories for realistic scenarios
- Mocks: Minimal - only external dependencies (payment gateways, email services)
- Coverage: One test per major feature capability
- CI/CD: âœ… Run on every commit

**Feature Test vs Integration Test**:
- **Feature Test**: Tests complete feature as user experiences it (multi-screen, multi-action, user perspective)
- **Integration Test**: Tests component interactions (single operation, multi-component, developer perspective)

**When to Write**:
- âœ… For each major feature
- âœ… For CRUD operations per entity
- âœ… For complete search/filter functionality
- âœ… For import/export features
- âœ… For authentication flows
- âœ… For payment/purchase flows

**When NOT to Write**:
- âŒ Single functions (use unit tests)
- âŒ Single components (use component tests)
- âŒ Component-only integration (use integration tests)

---

### **5. Workflow Tests (User Journey Validation)**

**Purpose**: Test complete user workflows end-to-end, validating sequences of actions users perform to accomplish goals across multiple features.

**Definition**: A workflow is a sequence of actions a user takes to accomplish a specific goal. Examples:
- **Onboarding Workflow**: First-time user adds first item and experiences success
- **Inventory Management Workflow**: Scan item â†’ review â†’ categorize â†’ add to inventory
- **Data Backup Workflow**: Export inventory â†’ transfer to new device â†’ import â†’ verify
- **Purchase Workflow**: Browse â†’ select â†’ pay â†’ receive confirmation â†’ verify purchase

**Scope**:
- End-to-end user journeys across multiple features
- Real-world user scenarios
- Complete sequences from start to finish
- Success paths (user achieves goal)
- Error paths (user recovers from failures)
- Edge cases (unlikely but possible scenarios)

**Characteristics**:
- Duration: 5-10 seconds per test
- Scope: Complete user journey, typically across multiple features
- Realism: Mimics real user behavior patterns
- Data: Realistic scenarios with test data factories
- Mocks: Minimal - real database, real navigation, real APIs in test environment
- Coverage: One test per critical user journey
- CI/CD: âœ… Run on every PR (important but slower)

**Workflow Test vs Feature Test**:
- **Workflow Test**: Tests user journey across multiple features (multi-feature, start-to-finish goal)
- **Feature Test**: Tests complete feature in isolation (single feature, internal completeness)

**Critical Workflows to Test**:
- âœ… **Onboarding**: New user's first successful interaction
- âœ… **Core Business Flow**: Primary value proposition workflows
- âœ… **Data Portability**: Export/import across devices/versions
- âœ… **Purchase/Upgrade**: Conversion-critical workflows
- âœ… **Error Recovery**: Critical failure and recovery paths

**When NOT to Write**:
- âŒ Simple feature flows (use feature tests)
- âŒ Unit functionality (use unit tests)
- âŒ Single screen behavior (use component tests)
- âŒ Too many - 1-2 per critical path is sufficient

---

### **6. System Tests (Platform & Performance)**

**Purpose**: Test entire application with platform integration, performance benchmarks, and system-level concerns.

**Scope**:
- Platform features (camera, storage, geolocation, notifications)
- OS integration (iOS/Android specific features, Windows/Mac/Linux differences)
- Real device testing (simulators and physical devices)
- Performance under load
- Memory usage patterns
- Startup time and app launch
- Battery consumption (mobile)
- Network condition handling (offline, slow, intermittent)

**Characteristics**:
- Full app with real dependencies
- Platform-specific implementations
- Slowest automated tests (10-30s)
- Run on real devices or high-fidelity emulators/simulators
- CI/CD: âš ï¸ Run on release builds or scheduled runs (not every commit)

**When to Write**:
- âœ… Platform-specific features (camera, GPS, file system)
- âœ… Performance benchmarks (startup, critical paths)
- âœ… Memory leak detection
- âœ… Cross-platform compatibility verification
- âœ… Real device validation

**When NOT to Write**:
- âŒ Business logic (use unit tests)
- âŒ UI components (use component tests)
- âŒ Standard flows (use integration/feature tests)

---

### **7. E2E Tests (Production Validation)**

**Purpose**: Test complete user scenarios in production-like environment with real devices, real network, and real services.

**Scope**:
- Real devices (physical hardware)
- Real network conditions (not mocked)
- Real app stores (for in-app purchases)
- Multiple device types and OS versions
- Production API endpoints
- Real third-party services (Stripe, Firebase, Auth0, etc.)
- Critical user paths only

**Characteristics**:
- Slowest tests (30s+ per test)
- External dependencies
- Flakiest due to network, timing, environment issues
- Requires special test infrastructure
- CI/CD: â³ Run manually before release or scheduled (daily/weekly)
- Expensive to maintain

**When to Write**:
- âœ… **Critical user journeys** that must absolutely work
- âœ… **Smoke tests** for production deployments
- âœ… **Purchase flows** with real payment systems
- âœ… **Cross-device synchronization**
- âœ… **Real-time features** with production WebSockets

**When NOT to Write**:
- âŒ Anything that can be tested at lower levels
- âŒ Edge cases (use unit/integration tests)
- âŒ Error handling (use unit/integration tests)
- âŒ Comprehensive coverage (use lower-level tests)

**E2E Test Philosophy**: 
> "E2E tests should give you confidence that your most critical user journeys work in production. They should not be your primary testing strategy."

---

## ğŸ“Š Test Coverage Strategy

### **Coverage Requirements by Test Type**

| Test Type | Target Coverage | Test Suite % | Rationale |
|-----------|-----------------|--------------|-----------|
| **Unit Tests** | 90%+ | ~25% | Foundation - every function tested |
| **Component Tests** | 80%+ | ~25% | All UI components tested |
| **Integration Tests** | 70%+ | ~20% | Key integrations validated |
| **Feature Tests** | 70%+ | ~15% | Complete features validated |
| **Workflow Tests** | 60%+ | ~10% | Critical flows only |
| **System Tests** | 50%+ | ~4% | Platform-specific features |
| **E2E Tests** | 40%+ | ~1% | Smoke tests only |
| **Overall** | **85%+** | **100%** | **Minimum acceptable quality** |

### **Coverage Goals**

**Minimum (MVP Launch)**:
- Overall: 85%
- Unit: 90%
- Component: 80%
- Integration: 70%
- Feature: 70%
- Critical workflows: 100% (all critical flows tested)

**Target (Production Quality)**:
- Overall: 90%
- Unit: 95%
- Component: 90%
- Integration: 85%
- Feature: 80%
- Workflow: 70%

**Exception Handling**:
- Generated code: Exclude from coverage
- Third-party libraries: Exclude from coverage
- Platform-specific code: Cover per platform
- Error paths: Must be covered
- Edge cases: Should be covered

---

## ğŸ“ Test Organization & Structure

### **Directory Structure Template**

```
project/
test/                                  # All tests
â”œâ”€â”€ unit/                             # Unit tests (~25%)
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ usecases/
â”‚   â”‚   â””â”€â”€ repositories/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ utils/
â”‚       â””â”€â”€ exceptions/
â”‚
â”œâ”€â”€ component/                        # Component/UI tests (~25%)
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ [component-name]_test.ext
â”‚   â””â”€â”€ pages/
â”‚       â””â”€â”€ [page-name]_test.ext
â”‚
â”œâ”€â”€ integration/                      # Integration tests (~20%)
â”‚   â”œâ”€â”€ database_test.ext
â”‚   â”œâ”€â”€ repository_test.ext
â”‚   â”œâ”€â”€ service_test.ext
â”‚   â””â”€â”€ api_test.ext
â”‚
â”œâ”€â”€ feature/                          # Feature tests (~15%)
â”‚   â”œâ”€â”€ item_management_feature_test.ext
â”‚   â”œâ”€â”€ search_feature_test.ext
â”‚   â”œâ”€â”€ import_export_feature_test.ext
â”‚   â””â”€â”€ auth_feature_test.ext
â”‚
â”œâ”€â”€ workflow/                         # Workflow tests (~10%)
â”‚   â”œâ”€â”€ onboarding_workflow_test.ext
â”‚   â”œâ”€â”€ core_business_workflow_test.ext
â”‚   â”œâ”€â”€ data_portability_workflow_test.ext
â”‚   â””â”€â”€ purchase_workflow_test.ext
â”‚
â”œâ”€â”€ system/                           # System tests (~4%)
â”‚   â”œâ”€â”€ performance_test.ext
â”‚   â”œâ”€â”€ memory_test.ext
â”‚   â””â”€â”€ platform_integration_test.ext
â”‚
â”œâ”€â”€ e2e/                              # E2E tests (~1%)
â”‚   â””â”€â”€ smoke_test.ext
â”‚
â”œâ”€â”€ helpers/                          # Test utilities
â”‚   â”œâ”€â”€ test_data_factory.ext
â”‚   â”œâ”€â”€ custom_matchers.ext
â”‚   â””â”€â”€ test_setup.ext
â”‚
â””â”€â”€ mocks/                            # Generated mocks
    â””â”€â”€ mock_[service].ext
```

### **Naming Conventions**

```
Unit Tests:
  Subject: functionName_WhenCondition_ExpectedResult
  Example: incrementQuantity_WhenItemExists_IncrementsByOne

Component Tests:
  Subject: ComponentName_WhenAction_ShouldResult
  Example: InventoryCard_WhenClicked_ShouldEmitItem

Integration Tests:
  Subject: Service_WhenAction_ShouldIntegrate
  Example: InventoryService_WhenSaving_ShouldPersistToDatabase

Feature Tests:
  Subject: Feature_WhenUserAction_ShouldCompleteFeature
  Example: ItemCreation_WhenValidData_ShouldCreateAndDisplayItem

Workflow Tests:
  Subject: Workflow_WhenUserGoal_ShouldCompleteJourney
  Example: Onboarding_WhenNewUser_ShouldCompleteFirstItemAddition
```

---

## ğŸ”§ CI/CD Integration

### **Test Execution Strategy**

```yaml
# Test Pipeline Configuration
Pre-commit (Local):
  - Unit tests only
  - Fast feedback (< 30 seconds)
  - Run on changed files only

Pre-push (Local):
  - Unit tests
  - Component tests
  - Integration tests
  - Full suite (< 5 minutes)

Pull Request (CI):
  - All test types except E2E
  - Coverage threshold check
  - Parallel execution
  - Full suite (< 10 minutes)

Main Branch (CI):
  - All test types including E2E
  - Full coverage report
  - Performance benchmarks
  - Daily scheduled run

Release (CD):
  - E2E smoke tests mandatory
  - System tests on real devices
  - Manual approval gate
  - Full regression suite
```

### **CI/CD Configuration Template**

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Environment
        uses: actions/setup-[language]@v3
        with:
          [language]-version: '[VERSION]'
      
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      
      - name: Run Unit Tests
        run: [TEST_COMMAND] test/unit/ --coverage
      
      - name: Check Coverage Threshold
        run: |
          # Fail if coverage below 90%
          if [ $COVERAGE -lt 90 ]; then
            echo "Unit test coverage below 90%"
            exit 1
          fi
      
      - name: Upload Coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info

  component-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Environment
        uses: actions/setup-[language]@v3
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      - name: Run Component Tests
        run: [TEST_COMMAND] test/component/

  integration-tests:
    runs-on: ubuntu-latest
    services:
      database:
        image: postgres:latest
        env:
          POSTGRES_PASSWORD: test
    steps:
      - uses: actions/checkout@v3
      - name: Setup Environment
        uses: actions/setup-[language]@v3
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      - name: Run Integration Tests
        run: [TEST_COMMAND] test/integration/

  feature-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, component-tests]
    steps:
      - uses: actions/checkout@v3
      - name: Setup Environment
        uses: actions/setup-[language]@v3
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      - name: Run Feature Tests
        run: [TEST_COMMAND] test/feature/

  workflow-tests:
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v3
      - name: Setup Environment
        uses: actions/setup-[language]@v3
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      - name: Run Workflow Tests
        run: [TEST_COMMAND] test/workflow/

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [feature-tests, workflow-tests]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      - name: Setup Environment
        uses: actions/setup-[language]@v3
      - name: Install Dependencies
        run: [INSTALL_COMMAND]
      - name: Run E2E Tests
        run: [TEST_COMMAND] test/e2e/
```

---

## ğŸ› ï¸ Implementation Guidelines

### **Writing Testable Code**

```
Good Testable Code:
âœ… Pure functions when possible
âœ… Dependency injection
âœ… Single responsibility
âœ… Clear inputs and outputs
âœ… Avoid global state
âœ… Use interfaces/abstractions
âœ… Separate logic from side effects

Bad for Testing:
âŒ Global variables
âŒ Static methods with hidden dependencies
âŒ Tight coupling
âŒ Mixing logic with I/O
âŒ Large functions with multiple responsibilities
âŒ Direct instantiation of dependencies
```

### **Test Data Management**

```
Test Factories:
- Create consistent test data
- Allow easy customization
- Keep tests readable
- Prevent duplication

Test Fixtures:
- Load from files for complex data
- Version control test data
- Reset between tests

Database Strategy:
- Use in-memory database for speed
- Reset database between test suites
- Use transactions to rollback changes
- Seed minimal required data
```

### **Mocking Strategy**

```
When to Mock:
âœ… External APIs
âœ… Databases (for unit tests)
âœ… File systems
âœ… Time/randomness
âœ… Third-party services

When NOT to Mock:
âŒ Your own business logic
âŒ Internal functions
âŒ Test subjects themselves
âŒ What you're actually testing

Mock Best Practices:
- Mock interfaces, not concrete classes
- Keep mocks simple
- Verify mock interactions
- Use mocking libraries (Mockito, Jest, Moq)
```

---

## ğŸ” Test Debugging & Maintenance

### **Common Test Issues**

```
Flaky Tests:
- Use explicit waits, not sleep()
- Reset state between tests
- Avoid race conditions
- Use test isolation
- Fix root cause, don't retry

Slow Tests:
- Mock external dependencies
- Use in-memory databases
- Run tests in parallel
- Remove unnecessary setup
- Profile test execution

Brittle Tests:
- Test behavior, not implementation
- Use semantic selectors
- Avoid test duplication
- Keep tests focused
- Refactor tests regularly
```

### **Test Maintenance Checklist**

```
- [ ] Tests run in under 5 minutes
- [ ] Coverage maintained above threshold
- [ ] No flaky tests
- [ ] Tests are readable and documented
- [ ] Mock objects are up to date
- [ ] Test data is realistic
- [ ] CI/CD pipeline passes consistently
- [ ] Test failures provide clear messages
- [ ] Tests are refactored with code changes
- [ ] Performance benchmarks are tracked
```

---

## ğŸš€ Technology-Specific Implementations

For concrete code examples and framework-specific implementations, see:

**ğŸ“„ `_TESTS-TECH-SPECIFIC.md`** - Complete testing implementations for:
- Flutter/Dart
- React/TypeScript
- Vue/JavaScript
- Angular/TypeScript
- Node.js/Express
- .NET/C#

Each technology section includes:
- Setup instructions
- All 7 test type implementations
- Framework-specific tools and libraries
- CI/CD configuration examples
- Best practices and patterns

---

## ğŸ“ˆ Test Metrics & Reporting

### **Key Metrics to Track**

```
Coverage Metrics:
- Line coverage
- Branch coverage
- Function coverage
- Statement coverage

Quality Metrics:
- Test execution time
- Number of flaky tests
- Test failure rate
- Bug escape rate
- Test maintenance time

Performance Metrics:
- Test suite execution time
- Performance regression detection
- Memory usage during tests
- CPU usage during tests
```

### **Reporting Tools**

```
- Coverage reports (lcov, cobertura)
- Test dashboards (Allure, ReportPortal)
- CI/CD integration (GitHub Actions, Jenkins)
- Performance tracking (custom benchmarks)
```

---

### **Quick Reference: Choosing the Right Test Type**

```
Question: What am I testing?

â”œâ”€ A single function/method? â†’ Unit Test
â”œâ”€ A UI component? â†’ Component Test
â”œâ”€ Multiple components working together? â†’ Integration Test
â”œâ”€ A complete user-facing feature? â†’ Feature Test
â”œâ”€ A multi-feature user journey? â†’ Workflow Test
â”œâ”€ Platform-specific functionality? â†’ System Test
â””â”€ Critical production workflow? â†’ E2E Test

Question: How critical is this?

â”œâ”€ Core business logic? â†’ Unit + Feature tests
â”œâ”€ User-visible feature? â†’ Component + Feature tests
â”œâ”€ Critical user journey? â†’ Workflow + E2E tests
â””â”€ Platform integration? â†’ System tests
```

---

**Guide Version**: 2.0 (Universal)  
**Last Updated**: 2025-12-08  
**Framework**: Universal - All Technologies  
**Maintainer**: [MAINTAINER_NAME]

---

*This universal testing guide provides technology-agnostic testing strategy and principles. For concrete implementations in your specific technology stack, refer to `_TESTS-TECH-SPECIFIC.md`.*